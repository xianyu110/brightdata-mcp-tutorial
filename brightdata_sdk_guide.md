# Bright Data Python SDK å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [SDKå®‰è£…ä¸é…ç½®](#sdkå®‰è£…ä¸é…ç½®)
3. [æ ¸å¿ƒåŠŸèƒ½è¯¦è§£](#æ ¸å¿ƒåŠŸèƒ½è¯¦è§£)
4. [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)
5. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
6. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## å¿«é€Ÿå¼€å§‹

### å®‰è£…SDK

```bash
pip install brightdata-sdk
```

### åŸºç¡€ç¤ºä¾‹

```python
from brightdata import bdclient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = bdclient(api_token="your_api_token")

# æ‰§è¡Œæœç´¢
results = client.search("Python web scraping")

# è§£æç»“æœ
print(client.parse_content(results))
```

## SDKå®‰è£…ä¸é…ç½®

### 1. å®‰è£…è¦æ±‚

- Python 3.8+
- ä¾èµ–åŒ…ä¼šè‡ªåŠ¨å®‰è£…ï¼šrequests, aiohttp, beautifulsoup4, openaiç­‰

### 2. å®¢æˆ·ç«¯é…ç½®

```python
from brightdata import bdclient

# åŸºç¡€é…ç½®
client = bdclient(api_token="your_api_token")

# é«˜çº§é…ç½®
client = bdclient(
    api_token="your_api_token",
    auto_create_zones=True,      # è‡ªåŠ¨åˆ›å»ºzones
    web_unlocker_zone="custom_web_zone",
    serp_zone="custom_serp_zone"
)
```

### 3. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
BRIGHTDATA_API_TOKEN=your_api_token
```

ç„¶ååœ¨ä»£ç ä¸­ï¼š

```python
client = bdclient()  # è‡ªåŠ¨ä».envè¯»å–
```

## æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. ç½‘é¡µæœç´¢ - search()

```python
# å•ä¸ªæœç´¢
result = client.search("best laptops 2024")

# æ‰¹é‡æœç´¢
queries = ["Python tutorial", "Web scraping", "API integration"]
results = client.search(query=queries)

# é«˜çº§å‚æ•°
result = client.search(
    query="machine learning",
    search_engine="google",    # æˆ– "bing", "yandex"
    country="us",             # ä¸¤å­—æ¯å›½å®¶ä»£ç 
    data_format="markdown",   # æˆ– "screenshot", "json"
    async_request=True,       # å¼‚æ­¥å¤„ç†
    max_workers=10,          # å¹¶å‘æ•°
    timeout=30               # è¶…æ—¶æ—¶é—´
)
```

### 2. ç½‘é¡µæŠ“å– - scrape()

```python
# å•ä¸ªURL
result = client.scrape("https://example.com")

# å¤šä¸ªURLï¼ˆå¹¶å‘å¤„ç†ï¼‰
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]
results = client.scrape(url=urls)

# é«˜çº§å‚æ•°
result = client.scrape(
    url="https://example.com",
    country="us",
    data_format="markdown",
    async_request=True
)

# ä¿å­˜ç»“æœ
client.download_content(result, filename="scrape_results.json")
```

### 3. ç½‘ç«™çˆ¬å– - crawl()

```python
# çˆ¬å–æ•´ä¸ªç½‘ç«™ï¼ˆåŒ…æ‹¬å­é¡µé¢ï¼‰
result = client.crawl(
    url="https://example.com/",
    depth=2,                              # çˆ¬å–æ·±åº¦
    filter="/product/",                   # åªçˆ¬å–åŒ…å«æ­¤è·¯å¾„çš„URL
    exclude_filter="/ads/",               # æ’é™¤åŒ…å«æ­¤è·¯å¾„çš„URL
    custom_output_fields=["markdown", "url", "page_title"]
)

print(f"çˆ¬å–ä»»åŠ¡ID: {result['snapshot_id']}")
```

### 4. LinkedInæ•°æ®é‡‡é›†

#### æœç´¢åŠŸèƒ½

```python
# æœç´¢èŒä½
jobs = client.search_linkedin.jobs(
    location="San Francisco",
    keyword="Python developer",
    country="US",
    time_range="Past month",
    job_type="Full-time"
)

# æœç´¢ä¸ªäººèµ„æ–™
profiles = client.search_linkedin.profiles(
    first_names=["John", "Jane"],
    last_names=["Smith", "Doe"]
)

# æœç´¢å¸–å­
posts = client.search_linkedin.posts(
    profile_url="https://www.linkedin.com/in/username",
    start_date="2024-01-01T00:00:00.000Z",
    end_date="2024-12-31T23:59:59.000Z"
)
```

#### æŠ“å–åŠŸèƒ½

```python
# æŠ“å–å…¬å¸ä¿¡æ¯
company_urls = [
    "https://www.linkedin.com/company/bright-data",
    "https://www.linkedin.com/company/microsoft"
]
companies = client.scrape_linkedin.companies(company_urls)

# æŠ“å–èŒä½ä¿¡æ¯
job_urls = ["https://www.linkedin.com/jobs/view/123456789"]
jobs = client.scrape_linkedin.jobs(job_urls)

# æŠ“å–ä¸ªäººèµ„æ–™
profile_url = "https://www.linkedin.com/in/username"
profile = client.scrape_linkedin.profiles(profile_url)
```

### 5. ChatGPTé›†æˆ

```python
# å•ä¸ªæç¤º
result = client.search_chatGPT(
    prompt="Explain quantum computing in simple terms"
)

# å¤šä¸ªæç¤ºå’Œåç»­é—®é¢˜
result = client.search_chatGPT(
    prompt=[
        "What are the top 3 programming languages in 2024?",
        "Best practices for web scraping",
        "How to build an API"
    ],
    additional_prompt=[
        "Can you provide code examples?",
        "What about legal considerations?",
        "Which frameworks do you recommend?"
    ]
)

# ä¸‹è½½ç»“æœ
client.download_content(result, filename="chatgpt_responses.json")
```

### 6. æµè§ˆå™¨è¿æ¥

```python
from playwright.sync_api import Playwright, sync_playwright

# é…ç½®æµè§ˆå™¨å‡­æ®
client = bdclient(
    api_token="your_api_token",
    browser_username="username-zone-browser_zone1",
    browser_password="your_password"
)

def scrape_with_browser(playwright: Playwright):
    # è¿æ¥åˆ°Bright Dataçš„æµè§ˆå™¨
    browser = playwright.chromium.connect_over_cdp(client.connect_browser())
    
    try:
        page = browser.new_page()
        page.goto("https://example.com", timeout=120000)
        
        # æ‰§è¡Œæµè§ˆå™¨æ“ä½œ
        content = page.content()
        title = page.title()
        
        # æˆªå›¾
        page.screenshot(path="screenshot.png")
        
        return {"title": title, "content": content}
    finally:
        browser.close()

# è¿è¡Œ
with sync_playwright() as playwright:
    result = scrape_with_browser(playwright)
```

## å®æˆ˜ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç”µå•†ä»·æ ¼ç›‘æ§

```python
import json
from datetime import datetime

def monitor_product_prices(product_urls):
    """ç›‘æ§äº§å“ä»·æ ¼"""
    client = bdclient(api_token="your_api_token")
    
    results = []
    for url in product_urls:
        try:
            # æŠ“å–äº§å“é¡µé¢
            data = client.scrape(
                url=url,
                data_format="markdown"
            )
            
            # è§£æå†…å®¹
            parsed = client.parse_content(data)
            
            results.append({
                "url": url,
                "timestamp": datetime.now().isoformat(),
                "data": parsed
            })
            
        except Exception as e:
            print(f"é”™è¯¯æŠ“å– {url}: {e}")
    
    # ä¿å­˜ç»“æœ
    with open("price_monitor.json", "w") as f:
        json.dump(results, f, indent=2)
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
products = [
    "https://www.amazon.com/dp/B08N5WRWNW",
    "https://www.amazon.com/dp/B09G9FPHY6"
]
monitor_product_prices(products)
```

### ç¤ºä¾‹2ï¼šæ–°é—»èšåˆå™¨

```python
def aggregate_news(topics):
    """èšåˆå¤šä¸ªä¸»é¢˜çš„æ–°é—»"""
    client = bdclient(api_token="your_api_token")
    
    all_news = {}
    
    for topic in topics:
        # æœç´¢æ–°é—»
        search_query = f"{topic} news {datetime.now().year}"
        results = client.search(
            query=search_query,
            search_engine="google",
            data_format="markdown"
        )
        
        # è§£æç»“æœ
        parsed = client.parse_content(results)
        all_news[topic] = parsed
    
    return all_news

# ä½¿ç”¨ç¤ºä¾‹
topics = ["AI technology", "climate change", "space exploration"]
news = aggregate_news(topics)
```

### ç¤ºä¾‹3ï¼šç«å“åˆ†æ

```python
def analyze_competitors(competitor_domains):
    """åˆ†æç«äº‰å¯¹æ‰‹ç½‘ç«™"""
    client = bdclient(api_token="your_api_token")
    
    analysis = {}
    
    for domain in competitor_domains:
        # çˆ¬å–ç½‘ç«™
        crawl_result = client.crawl(
            url=f"https://{domain}",
            depth=2,
            filter="/product/",
            custom_output_fields=["markdown", "url", "page_title"]
        )
        
        snapshot_id = crawl_result['snapshot_id']
        
        # ç­‰å¾…çˆ¬å–å®Œæˆå¹¶ä¸‹è½½ç»“æœ
        # æ³¨æ„ï¼šcrawlæ˜¯å¼‚æ­¥æ“ä½œï¼Œå¯èƒ½éœ€è¦ç­‰å¾…
        analysis[domain] = {
            "snapshot_id": snapshot_id,
            "crawl_time": datetime.now().isoformat()
        }
    
    return analysis
```

## é«˜çº§åŠŸèƒ½

### 1. å¼‚æ­¥æ“ä½œ

```python
import asyncio

async def async_scraping():
    """å¼‚æ­¥æŠ“å–ç¤ºä¾‹"""
    client = bdclient(api_token="your_api_token")
    
    urls = ["https://example1.com", "https://example2.com", "https://example3.com"]
    
    # å¼‚æ­¥æŠ“å–
    results = await client.scrape(
        url=urls,
        async_request=True,
        max_workers=5
    )
    
    return results

# è¿è¡Œå¼‚æ­¥å‡½æ•°
results = asyncio.run(async_scraping())
```

### 2. Zoneç®¡ç†

```python
# åˆ—å‡ºæ‰€æœ‰zones
zones = client.list_zones()
for zone in zones:
    print(f"Zone: {zone['name']}, Type: {zone['type']}")

# ä½¿ç”¨ç‰¹å®šzone
result = client.scrape(
    url="https://example.com",
    zone="custom_unlocker_zone"
)
```

### 3. æ•°æ®å¤„ç†å·¥å…·

```python
# ä¸‹è½½å¿«ç…§
client.download_snapshot(
    snapshot_id="s_abc123xyz",
    filename="snapshot_data.json"
)

# æ‰¹é‡ä¸‹è½½
snapshots = ["s_abc123", "s_def456", "s_ghi789"]
for snapshot in snapshots:
    client.download_snapshot(snapshot, f"{snapshot}.json")
```

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

```python
from brightdata import bdclient
import time

def safe_scrape(url, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„å®‰å…¨æŠ“å–"""
    client = bdclient(api_token="your_api_token")
    
    for attempt in range(max_retries):
        try:
            result = client.scrape(url)
            return result
            
        except Exception as e:
            error_type = type(e).__name__
            
            if "timeout" in str(e).lower():
                print(f"è¶…æ—¶é”™è¯¯ï¼Œé‡è¯• {attempt + 1}/{max_retries}")
                time.sleep(5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                
            elif "authentication" in str(e).lower():
                print("è®¤è¯é”™è¯¯ï¼šè¯·æ£€æŸ¥APIä»¤ç‰Œ")
                raise
                
            elif "rate limit" in str(e).lower():
                print("é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…60ç§’...")
                time.sleep(60)
                
            else:
                print(f"æœªçŸ¥é”™è¯¯: {error_type} - {e}")
                if attempt == max_retries - 1:
                    raise
    
    return None
```

## æœ€ä½³å®è·µ

### 1. APIä»¤ç‰Œå®‰å…¨

```python
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡è·å–ä»¤ç‰Œ
api_token = os.getenv("BRIGHTDATA_API_TOKEN")
if not api_token:
    raise ValueError("æœªè®¾ç½®BRIGHTDATA_API_TOKENç¯å¢ƒå˜é‡")

client = bdclient(api_token=api_token)
```

### 2. æ—¥å¿—è®°å½•

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filename='brightdata.log'
)

logger = logging.getLogger('brightdata_app')

def scrape_with_logging(url):
    """å¸¦æ—¥å¿—çš„æŠ“å–"""
    logger.info(f"å¼€å§‹æŠ“å–: {url}")
    
    try:
        client = bdclient(api_token="your_api_token")
        result = client.scrape(url)
        logger.info(f"æˆåŠŸæŠ“å–: {url}")
        return result
        
    except Exception as e:
        logger.error(f"æŠ“å–å¤±è´¥ {url}: {e}")
        raise
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# æ‰¹é‡æ“ä½œä¼˜åŒ–
def efficient_batch_scraping(urls, batch_size=10):
    """é«˜æ•ˆçš„æ‰¹é‡æŠ“å–"""
    client = bdclient(api_token="your_api_token")
    
    all_results = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(urls), batch_size):
        batch = urls[i:i + batch_size]
        
        # å¹¶å‘æŠ“å–å½“å‰æ‰¹æ¬¡
        results = client.scrape(
            url=batch,
            max_workers=min(batch_size, 10),  # é™åˆ¶å¹¶å‘æ•°
            timeout=60
        )
        
        all_results.extend(results)
        
        # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…è§¦å‘é™åˆ¶
        if i + batch_size < len(urls):
            time.sleep(2)
    
    return all_results
```

## æ€»ç»“

Bright Data Python SDKæä¾›äº†å¼ºå¤§è€Œçµæ´»çš„ç½‘é¡µæ•°æ®é‡‡é›†èƒ½åŠ›ï¼š

1. **ç®€å•æ˜“ç”¨** - å‡ è¡Œä»£ç å³å¯å¼€å§‹é‡‡é›†
2. **åŠŸèƒ½ä¸°å¯Œ** - æ”¯æŒæœç´¢ã€æŠ“å–ã€çˆ¬å–ã€LinkedInæ•°æ®ç­‰
3. **é«˜æ€§èƒ½** - è‡ªåŠ¨å¹¶å‘å¤„ç†ï¼Œæ”¯æŒå¼‚æ­¥æ“ä½œ
4. **å¯é ç¨³å®š** - å†…ç½®é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†

è®°ä½å§‹ç»ˆéµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtæ–‡ä»¶ï¼Œåˆç†ä½¿ç”¨é‡‡é›†åŠŸèƒ½ã€‚

## èµ„æºé“¾æ¥

- [å®˜æ–¹æ–‡æ¡£](https://docs.brightdata.com/api-reference/SDK)
- [GitHubä»“åº“](https://github.com/brightdata/bright-data-sdk-python)
- [APIå‚è€ƒ](https://brightdata.com/cp/api_reference)
- [æŠ€æœ¯æ”¯æŒ](https://brightdata.com/support)
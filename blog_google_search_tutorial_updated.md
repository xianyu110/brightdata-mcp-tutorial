# ä½¿ç”¨Pythonå’ŒBright Data SDKå®æ—¶æŠ“å–Googleæœç´¢ç»“æœï¼šå®Œæ•´å®æˆ˜æŒ‡å—

> åœ¨æ•°æ®é©±åŠ¨çš„æ—¶ä»£ï¼Œè·å–å®æ—¶æœç´¢ç»“æœå¯¹äºå¸‚åœºç ”ç©¶ã€ç«äº‰åˆ†æå’ŒSEOä¼˜åŒ–è‡³å…³é‡è¦ã€‚æœ¬æ–‡å°†é€šè¿‡å®æˆ˜æ¡ˆä¾‹ï¼Œè¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨Pythonå’ŒBright Data SDKé«˜æ•ˆåœ°æŠ“å–Googleæœç´¢ç»“æœã€‚

## ç›®å½•

1. [ä¸ºä»€ä¹ˆé€‰æ‹©Bright Dataï¼Ÿ](#ä¸ºä»€ä¹ˆé€‰æ‹©bright-data)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)
4. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
5. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
6. [æ³¨æ„äº‹é¡¹](#æ³¨æ„äº‹é¡¹)

## ä¸ºä»€ä¹ˆé€‰æ‹©Bright Dataï¼Ÿ

ç»è¿‡å®é™…æµ‹è¯•ï¼ŒBright Data SDKåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼š

âœ… **100%æˆåŠŸç‡** - åœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­ï¼Œæ‰€æœ‰è¯·æ±‚éƒ½æˆåŠŸè¿”å›æ•°æ®  
âœ… **è‡ªåŠ¨å¤„ç†åçˆ¬è™«** - æ— éœ€æ‹…å¿ƒéªŒè¯ç å’ŒIPå°é”  
âœ… **å…¨çƒè¦†ç›–** - æ”¯æŒç¾å›½ã€è‹±å›½ã€åŠ æ‹¿å¤§ã€æ¾³å¤§åˆ©äºšç­‰å¤šä¸ªåœ°åŒº  
âœ… **é«˜æ€§èƒ½** - å¹³å‡å“åº”å¤§å°çº¦1MBï¼ŒåŒ…å«å®Œæ•´æœç´¢ç»“æœ  

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…SDK

```bash
pip install brightdata-sdk
```

### 2. åŸºç¡€ä½¿ç”¨

```python
from brightdata import bdclient

# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ‚¨çš„APIä»¤ç‰Œï¼‰
client = bdclient(api_token="your_api_token")

# æ‰§è¡Œæœç´¢
results = client.search("Python web scraping tutorial 2024")

# ä¿å­˜ç»“æœ
if isinstance(results, str):
    print(f"æˆåŠŸï¼è·å¾— {len(results):,} å­—ç¬¦çš„HTMLå“åº”")
    with open("search_results.html", "w", encoding="utf-8") as f:
        f.write(results)
```

### å®é™…æµ‹è¯•ç»“æœ

åœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­ï¼Œæœç´¢"Python web scraping tutorial 2024"è¿”å›äº†**481,161å­—ç¬¦**çš„HTMLå†…å®¹ï¼ŒåŒ…å«äº†å®Œæ•´çš„Googleæœç´¢ç»“æœé¡µé¢ã€‚

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ‰¹é‡æœç´¢å…³é”®è¯ï¼ˆå·²æµ‹è¯•ï¼‰

```python
from brightdata import bdclient
import json
import time
from datetime import datetime

class GoogleSearchScraper:
    def __init__(self, api_token):
        self.client = bdclient(api_token=api_token)
        
    def batch_search(self, queries, delay=2):
        """æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯"""
        results = []
        
        for i, query in enumerate(queries, 1):
            print(f"[{i}/{len(queries)}] æœç´¢: {query}")
            
            try:
                # æ‰§è¡Œæœç´¢
                raw_results = self.client.search(query)
                
                results.append({
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "content_length": len(raw_results) if isinstance(raw_results, str) else 0
                })
                
                print(f"âœ… æˆåŠŸ - {len(raw_results):,} å­—ç¬¦")
                
            except Exception as e:
                results.append({
                    "query": query,
                    "status": "error",
                    "error": str(e)
                })
                print(f"âŒ å¤±è´¥: {e}")
            
            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(queries):
                time.sleep(delay)
        
        return results

# ä½¿ç”¨ç¤ºä¾‹
scraper = GoogleSearchScraper("your_api_token")
keywords = [
    "machine learning algorithms",
    "deep learning frameworks", 
    "neural network architectures"
]

results = scraper.batch_search(keywords)
```

**å®æµ‹ç»“æœ**ï¼š
- "machine learning algorithms": 2,094,108 å­—ç¬¦ âœ…
- "deep learning frameworks": 884,689 å­—ç¬¦ âœ…
- "neural network architectures": 942,299 å­—ç¬¦ âœ…
- **æˆåŠŸç‡**: 100% (3/3)
- **æ€»è€—æ—¶**: çº¦30ç§’

### æ¡ˆä¾‹2ï¼šå¤šåœ°åŒºæœç´¢æ¯”è¾ƒï¼ˆå·²æµ‹è¯•ï¼‰

```python
def search_multiple_regions(query, countries):
    """åœ¨å¤šä¸ªåœ°åŒºæœç´¢åŒä¸€å…³é”®è¯"""
    client = bdclient(api_token="your_api_token")
    regional_results = {}
    
    for country in countries:
        print(f"æœç´¢åœ°åŒº: {country}")
        
        try:
            results = client.search(
                query=query,
                search_engine="google",
                country=country
            )
            
            regional_results[country] = {
                "success": True,
                "content_length": len(results),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            regional_results[country] = {
                "success": False,
                "error": str(e)
            }
        
        time.sleep(1)
    
    return regional_results

# æµ‹è¯•ä¸åŒåœ°åŒº
countries = ["us", "uk", "ca", "au"]
results = search_multiple_regions("AI technology trends", countries)
```

**å®æµ‹ç»“æœ**ï¼š
- ğŸ‡ºğŸ‡¸ ç¾å›½: 1,396,842 å­—ç¬¦
- ğŸ‡¬ğŸ‡§ è‹±å›½: 771,884 å­—ç¬¦
- ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§: 414,731 å­—ç¬¦
- ğŸ‡¦ğŸ‡º æ¾³å¤§åˆ©äºš: 806,124 å­—ç¬¦

ä¸åŒåœ°åŒºè¿”å›çš„æ•°æ®é‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç¾å›½å¸‚åœºçš„æœç´¢ç»“æœæœ€ä¸ºä¸°å¯Œã€‚

### æ¡ˆä¾‹3ï¼šç«äº‰å¯¹æ‰‹ç›‘æ§ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰

åŸºäºå®é™…æµ‹è¯•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ç«äº‰å¯¹æ‰‹çš„ç›‘æ§ç³»ç»Ÿï¼š

```python
from bs4 import BeautifulSoup

def analyze_competitors(html_content, competitors):
    """åˆ†ææœç´¢ç»“æœä¸­çš„ç«äº‰å¯¹æ‰‹"""
    soup = BeautifulSoup(html_content, 'html.parser')
    text_content = soup.get_text().lower()
    
    competitor_analysis = {}
    
    for competitor in competitors:
        domain = competitor['domain'].lower()
        name = competitor['name'].lower()
        
        # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
        occurrences = text_content.count(domain) + text_content.count(name)
        
        # æŸ¥æ‰¾é“¾æ¥ä½ç½®
        positions = []
        for i, link in enumerate(soup.find_all('a', href=True), 1):
            if domain in str(link).lower():
                positions.append(i)
        
        competitor_analysis[competitor['name']] = {
            'occurrences': occurrences,
            'found': occurrences > 0,
            'positions': positions[:5]  # å‰5ä¸ªä½ç½®
        }
    
    return competitor_analysis

# å®é™…ä½¿ç”¨
competitors = [
    {"name": "ScrapingBee", "domain": "scrapingbee.com"},
    {"name": "Apify", "domain": "apify.com"},
    {"name": "Bright Data", "domain": "brightdata.com"}
]

# æœç´¢å¹¶åˆ†æ
results = client.search("web scraping services")
analysis = analyze_competitors(results, competitors)
```

**å®æµ‹å‘ç°**ï¼š
- **ScrapingBee**: 8æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬17ä½
- **Apify**: 8æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬14ä½
- **Bright Data**: 1æ¬¡å‡ºç°

## é«˜çº§åŠŸèƒ½

### 1. å¤„ç†å¤§é‡HTMLæ•°æ®

ç”±äºæœç´¢ç»“æœé€šå¸¸è¿”å›å¤§é‡HTMLï¼ˆå¹³å‡çº¦1MBï¼‰ï¼Œå»ºè®®ä½¿ç”¨æµå¼å¤„ç†ï¼š

```python
def process_large_html(html_content):
    """é«˜æ•ˆå¤„ç†å¤§å‹HTMLå†…å®¹"""
    # ä½¿ç”¨BeautifulSoupçš„å¿«é€Ÿè§£æå™¨
    soup = BeautifulSoup(html_content, 'lxml')  # éœ€è¦: pip install lxml
    
    # æå–å…³é”®ä¿¡æ¯
    search_results = []
    
    # æŸ¥æ‰¾æ‰€æœ‰æœç´¢ç»“æœå®¹å™¨
    for result in soup.select('div.g'):  # Googleæœç´¢ç»“æœçš„å…¸å‹é€‰æ‹©å™¨
        title_elem = result.select_one('h3')
        link_elem = result.select_one('a')
        desc_elem = result.select_one('div.VwiC3b')
        
        if title_elem and link_elem:
            search_results.append({
                'title': title_elem.get_text(strip=True),
                'url': link_elem.get('href', ''),
                'description': desc_elem.get_text(strip=True) if desc_elem else ''
            })
    
    return search_results
```

### 2. å¼‚æ­¥æ‰¹é‡å¤„ç†

å¯¹äºå¤§è§„æ¨¡æŠ“å–ï¼Œä½¿ç”¨å¼‚æ­¥å¤„ç†æé«˜æ•ˆç‡ï¼š

```python
import asyncio
import aiohttp

async def async_search(query, session):
    """å¼‚æ­¥æœç´¢å•ä¸ªæŸ¥è¯¢"""
    # æ³¨æ„ï¼šè¿™æ˜¯ç¤ºä¾‹ä»£ç ï¼Œå®é™…ä½¿ç”¨éœ€è¦é€‚é…Bright Data SDK
    try:
        # æ‰§è¡Œå¼‚æ­¥è¯·æ±‚
        result = await session.post(url, json={"query": query})
        return {"query": query, "success": True, "data": result}
    except Exception as e:
        return {"query": query, "success": False, "error": str(e)}

async def batch_search_async(queries):
    """å¼‚æ­¥æ‰¹é‡æœç´¢"""
    async with aiohttp.ClientSession() as session:
        tasks = [async_search(query, session) for query in queries]
        results = await asyncio.gather(*tasks)
    return results
```

### 3. æ•°æ®å¯¼å‡ºå’Œåˆ†æ

```python
import pandas as pd

def export_search_results(results, format='excel'):
    """å¯¼å‡ºæœç´¢ç»“æœä¸ºä¸åŒæ ¼å¼"""
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results)
    
    # æ·»åŠ åˆ†æåˆ—
    df['content_mb'] = df['content_length'] / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    df['success_rate'] = df['status'].apply(lambda x: 1 if x == 'success' else 0)
    
    # å¯¼å‡º
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    if format == 'excel':
        filename = f'search_results_{timestamp}.xlsx'
        df.to_excel(filename, index=False)
    elif format == 'csv':
        filename = f'search_results_{timestamp}.csv'
        df.to_csv(filename, index=False)
    
    # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    print("\nğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"æ€»æŸ¥è¯¢æ•°: {len(df)}")
    print(f"æˆåŠŸç‡: {df['success_rate'].mean():.1%}")
    print(f"å¹³å‡å“åº”å¤§å°: {df['content_mb'].mean():.2f} MB")
    
    return filename
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

åŸºäºå®é™…æµ‹è¯•ç»éªŒï¼š

### 1. è¯·æ±‚é—´éš”
```python
# å»ºè®®ï¼šæ¯ä¸ªè¯·æ±‚ä¹‹é—´ä¿æŒ2ç§’é—´éš”
time.sleep(2)
```

### 2. é”™è¯¯é‡è¯•
```python
def search_with_retry(query, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æœç´¢"""
    for attempt in range(max_retries):
        try:
            return client.search(query)
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"é‡è¯• {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {wait_time}ç§’...")
                time.sleep(wait_time)
            else:
                raise
```

### 3. å†…å­˜ä¼˜åŒ–
```python
# å¯¹äºå¤§é‡æ•°æ®ï¼Œä½¿ç”¨ç”Ÿæˆå™¨
def search_generator(queries):
    """ä½¿ç”¨ç”Ÿæˆå™¨èŠ‚çœå†…å­˜"""
    for query in queries:
        yield {
            'query': query,
            'result': client.search(query)
        }
        time.sleep(2)
```

## æ³¨æ„äº‹é¡¹

### âš ï¸ é‡è¦æé†’

1. **APIå“åº”æ ¼å¼**
   - SDKå½“å‰è¿”å›HTMLæ ¼å¼çš„å“åº”ï¼Œè€Œéç»“æ„åŒ–JSON
   - éœ€è¦ä½¿ç”¨HTMLè§£æå™¨ï¼ˆå¦‚BeautifulSoupï¼‰æå–æ•°æ®

2. **å“åº”å¤§å°**
   - å¹³å‡å“åº”å¤§å°çº¦1MBï¼ˆ40ä¸‡-200ä¸‡å­—ç¬¦ï¼‰
   - å»ºè®®å®ç°é€‚å½“çš„å­˜å‚¨å’Œå¤„ç†ç­–ç•¥

3. **è¯·æ±‚é™åˆ¶**
   - éµå®ˆAPIè°ƒç”¨é™é¢ï¼ˆå‰3ä¸ªæœˆæ¯æœˆ5000æ¬¡å…è´¹ï¼‰
   - å®æ–½è¯·æ±‚é—´éš”é¿å…è§¦å‘é™åˆ¶

4. **åˆè§„ä½¿ç”¨**
   - éµå®ˆGoogleæœåŠ¡æ¡æ¬¾
   - ä»…ç”¨äºåˆæ³•çš„æ•°æ®åˆ†æç›®çš„

## æ€»ç»“

é€šè¿‡æœ¬æ–‡çš„å®æˆ˜æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Bright Data SDKé«˜æ•ˆåœ°æŠ“å–Googleæœç´¢ç»“æœã€‚ä¸»è¦æ”¶è·åŒ…æ‹¬ï¼š

âœ… **SDKç®€å•æ˜“ç”¨** - å‡ è¡Œä»£ç å³å¯å¼€å§‹æŠ“å–  
âœ… **é«˜æˆåŠŸç‡** - æµ‹è¯•ä¸­100%æˆåŠŸç‡  
âœ… **ä¸°å¯Œçš„æ•°æ®** - æ¯æ¬¡è¯·æ±‚è¿”å›å®Œæ•´çš„æœç´¢ç»“æœ  
âœ… **å¤šåœ°åŒºæ”¯æŒ** - è½»æ¾è·å–ä¸åŒå›½å®¶çš„æœç´¢ç»“æœ  

æ— è®ºæ˜¯SEOåˆ†æã€å¸‚åœºç ”ç©¶è¿˜æ˜¯ç«å“ç›‘æ§ï¼ŒBright Data SDKéƒ½æä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚è®°ä½åˆç†ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œè®©æ•°æ®é©±åŠ¨æ‚¨çš„ä¸šåŠ¡å†³ç­–ã€‚

## ç›¸å…³èµ„æº

- ğŸ“¦ [å®Œæ•´ä»£ç ç¤ºä¾‹](https://github.com/your-repo/brightdata-google-search)
- ğŸ“š [Bright Dataå®˜æ–¹æ–‡æ¡£](https://docs.brightdata.com)
- ğŸ› ï¸ [Python SDK GitHub](https://github.com/brightdata/bright-data-sdk-python)
- ğŸ’¬ [æŠ€æœ¯æ”¯æŒ](https://brightdata.com/support)

---

*ä½œè€…ï¼šæŠ€æœ¯å®è·µå›¢é˜Ÿ*  
*æ›´æ–°æ—¥æœŸï¼š2024å¹´1æœˆ*  
*åŸºäºçœŸå®æµ‹è¯•æ•°æ®*  
*æ ‡ç­¾ï¼šPython, Web Scraping, Google Search, Bright Data, å®æˆ˜æ•™ç¨‹*
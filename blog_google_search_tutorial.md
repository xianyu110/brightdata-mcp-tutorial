# ä½¿ç”¨Pythonè°ƒç”¨Bright Data APIå®æ—¶æŠ“å–Googleæœç´¢ç»“æœ

> åœ¨æ•°æ®é©±åŠ¨çš„æ—¶ä»£ï¼Œè·å–å®æ—¶æœç´¢ç»“æœå¯¹äºå¸‚åœºç ”ç©¶ã€ç«äº‰åˆ†æå’Œå†…å®¹ä¼˜åŒ–è‡³å…³é‡è¦ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨Pythonå’ŒBright Data SDKé«˜æ•ˆåœ°æŠ“å–Googleæœç´¢ç»“æœã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©Bright Dataï¼Ÿ

Bright Dataæ˜¯å…¨çƒæ’åç¬¬ä¸€çš„ç½‘ç»œæ•°æ®å¹³å°ï¼Œä¸ºå‡ ä¸‡å®¶ç»„ç»‡æä¾›æ•°æ®éœ€æ±‚æ”¯æŒã€‚åŸºäºæˆ‘ä»¬çš„å®é™…æµ‹è¯•ï¼ŒBright Dataåœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°å“è¶Šï¼š

1. **100%æˆåŠŸç‡** - åœ¨æˆ‘ä»¬çš„æ‰€æœ‰æµ‹è¯•ä¸­ï¼Œæ¯ä¸ªè¯·æ±‚éƒ½æˆåŠŸè¿”å›æ•°æ®
2. **è‡ªåŠ¨å¤„ç†åçˆ¬è™«æœºåˆ¶** - æ— éœ€æ‹…å¿ƒéªŒè¯ç ã€IPå°é”ç­‰é—®é¢˜
3. **å…¨çƒä»£ç†ç½‘ç»œ** - æ”¯æŒç¾å›½ã€è‹±å›½ã€åŠ æ‹¿å¤§ã€æ¾³å¤§åˆ©äºšç­‰å¤šä¸ªåœ°åŒº
4. **é«˜æ€§èƒ½å¤„ç†** - å¹³å‡å“åº”åŒ…å«40ä¸‡-200ä¸‡å­—ç¬¦çš„å®Œæ•´æœç´¢ç»“æœ
5. **å…è´¹é¢åº¦** - å‰3ä¸ªæœˆå…è´¹ï¼Œæ¯æœˆ5,000æ¬¡è¯·æ±‚

## ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…Bright Data SDK

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å®˜æ–¹Python SDKï¼š

```bash
pip install brightdata-sdk
```

### 2. è·å–APIä»¤ç‰Œ

1. æ³¨å†Œ[Bright Dataè´¦æˆ·](https://brightdata.com)
2. åœ¨è´¦æˆ·è®¾ç½®ä¸­ç”ŸæˆAPIä»¤ç‰Œ
3. ç¡®ä¿ä»¤ç‰Œå…·æœ‰ç®¡ç†å‘˜æƒé™

![image-20250914105118072](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914105118072.png)

## åŸºç¡€å®ç°ï¼šæŠ“å–æœç´¢ç»“æœ

è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹ã€‚æ ¹æ®å®é™…æµ‹è¯•ï¼Œæœç´¢"Python web scraping tutorial 2024"è¿”å›äº†**481,161å­—ç¬¦**çš„HTMLå†…å®¹ï¼š

```python
from brightdata import bdclient
import json
from datetime import datetime

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = bdclient(api_token="your_api_token")

# æ‰§è¡Œæœç´¢
query = "Python web scraping best practices 2024"
print(f"æ­£åœ¨æœç´¢: {query}")
results = client.search(query)

# å¤„ç†ç»“æœ - åŸºäºå®é™…æµ‹è¯•ï¼ŒSDKè¿”å›HTMLæ ¼å¼
if isinstance(results, str):
    print(f"\nâœ… æœç´¢æˆåŠŸï¼")
    print(f"å“åº”ç±»å‹: HTML")
    print(f"å“åº”å¤§å°: {len(results):,} å­—ç¬¦")
    
    # ä¿å­˜HTMLç»“æœ
    with open("search_results.html", "w", encoding="utf-8") as f:
        f.write(results)
    
    # ä½¿ç”¨BeautifulSoupè§£æ
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(results, 'html.parser')
    
    # æå–æœç´¢ç»“æœæ ‡é¢˜
    titles = soup.find_all('h3')
    print(f"\næ‰¾åˆ° {len(titles)} ä¸ªæœç´¢ç»“æœæ ‡é¢˜")
    for i, title in enumerate(titles[:5], 1):
        print(f"{i}. {title.get_text()}")
else:
    print(f"æ„å¤–çš„è¿”å›ç±»å‹: {type(results)}")
```

![image-20250914105447208](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914105447208.png)

æŸ¥è¯¢æˆåŠŸå¹¶ä¿å­˜ä¸ºjsonï¼š

![image-20250914105751871](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914105751871.png)

## è¿›é˜¶åŠŸèƒ½ï¼šå®šåˆ¶åŒ–æœç´¢

### 1. æŒ‡å®šæœç´¢å¼•æ“å’Œåœ°åŒº

```python
def search_with_location(query, country="us", language="en"):
    """
    æ‰§è¡Œç‰¹å®šåœ°åŒºçš„æœç´¢
    
    Args:
        query: æœç´¢å…³é”®è¯
        country: å›½å®¶ä»£ç ï¼ˆå¦‚ us, uk, cnï¼‰
        language: è¯­è¨€ä»£ç ï¼ˆå¦‚ en, zhï¼‰
    """
    results = client.search(
        query=query,
        search_engine="google",
        country=country,
        data_format="markdown"  # è¿”å›Markdownæ ¼å¼ï¼Œä¾¿äºé˜…è¯»
    )
    
    return client.parse_content(results)

# æœç´¢ä¸åŒåœ°åŒºçš„ç»“æœ
us_results = search_with_location("AI technology trends", "us")
uk_results = search_with_location("AI technology trends", "uk")
ca_results = search_with_location("AI technology trends", "ca")
au_results = search_with_location("AI technology trends", "au")

# å®é™…æµ‹è¯•ç»“æœ - ä¸åŒåœ°åŒºå“åº”å¤§å°å·®å¼‚æ˜¾è‘—ï¼š
# ğŸ‡ºğŸ‡¸ ç¾å›½: 1,396,842 å­—ç¬¦
# ğŸ‡¬ğŸ‡§ è‹±å›½: 771,884 å­—ç¬¦
# ğŸ‡¨ğŸ‡¦ åŠ æ‹¿å¤§: 414,731 å­—ç¬¦
# ğŸ‡¦ğŸ‡º æ¾³å¤§åˆ©äºš: 806,124 å­—ç¬¦
```

![image-20250914110523805](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914110523805.png)

![image-20250914110537402](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914110537402.png)

### 2. æ‰¹é‡æœç´¢å¤„ç†

å½“éœ€è¦å¤„ç†å¤šä¸ªæœç´¢æŸ¥è¯¢æ—¶ï¼ŒSDKä¼šè‡ªåŠ¨è¿›è¡Œå¹¶å‘å¤„ç†ï¼š

```python
def batch_search(queries, max_workers=5):
    """
    æ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯
    
    Args:
        queries: æœç´¢å…³é”®è¯åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶å‘æ•°
    """
    print(f"å¼€å§‹æ‰¹é‡æœç´¢ {len(queries)} ä¸ªå…³é”®è¯...")
    
    # SDKè‡ªåŠ¨å¤„ç†å¹¶å‘
    results = client.search(
        query=queries,
        max_workers=max_workers,
        timeout=60
    )
    
    # å¤„ç†æ¯ä¸ªç»“æœ
    parsed_results = []
    for i, result in enumerate(results):
        parsed = client.parse_content(result)
        parsed_results.append({
            "query": queries[i],
            "timestamp": datetime.now().isoformat(),
            "results": parsed
        })
    
    return parsed_results

# æ‰¹é‡æœç´¢ç¤ºä¾‹
keywords = [
    "machine learning algorithms",
    "deep learning frameworks",
    "neural network architectures"
]

batch_results = batch_search(keywords)

# å®é™…æµ‹è¯•ç»“æœï¼š
# âœ… "machine learning algorithms": 2,094,108 å­—ç¬¦
# âœ… "deep learning frameworks": 884,689 å­—ç¬¦  
# âœ… "neural network architectures": 942,299 å­—ç¬¦
# æˆåŠŸç‡: 100% (3/3)
# æ€»è€—æ—¶: çº¦30ç§’
```

![image-20250914105933552](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914105933552.png)



![image-20250914105948679](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914105948679.png)

## å®æˆ˜æ¡ˆä¾‹ï¼šç«å“å…³é”®è¯ç›‘æ§

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå®é™…çš„åº”ç”¨åœºæ™¯ - ç›‘æ§ç«äº‰å¯¹æ‰‹çš„å…³é”®è¯æ’åï¼š

```python
import time
from datetime import datetime

class CompetitorKeywordMonitor:
    def __init__(self, api_token):
        self.client = bdclient(api_token=api_token)
        self.competitors = []
        
    def add_competitor(self, name, domain):
        """æ·»åŠ éœ€è¦ç›‘æ§çš„ç«äº‰å¯¹æ‰‹"""
        self.competitors.append({"name": name, "domain": domain})
        
    def monitor_keywords(self, keywords):
        """ç›‘æ§å…³é”®è¯æ’å"""
        results = {}
        
        for keyword in keywords:
            print(f"æ­£åœ¨æœç´¢: {keyword}")
            
            # æœç´¢ç»“æœ
            search_results = self.client.search(
                query=keyword,
                search_engine="google",
                data_format="markdown"
            )
            
            # è§£æç»“æœ
            parsed = self.client.parse_content(search_results)
            
            # æŸ¥æ‰¾ç«äº‰å¯¹æ‰‹æ’å
            rankings = self._extract_rankings(parsed)
            results[keyword] = {
                "timestamp": datetime.now().isoformat(),
                "rankings": rankings
            }
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
            
        return results
    
    def _extract_rankings(self, parsed_results):
        """ä»æœç´¢ç»“æœä¸­æå–æ’åä¿¡æ¯"""
        rankings = {}
        
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¿”å›çš„æ•°æ®ç»“æ„è¿›è¡Œè§£æ
        # ç¤ºä¾‹é€»è¾‘ï¼š
        if isinstance(parsed_results, dict) and 'text' in parsed_results:
            text = parsed_results['text'].lower()
            for competitor in self.competitors:
                if competitor['domain'].lower() in text:
                    # ç®€åŒ–çš„æ’åæå–é€»è¾‘
                    rankings[competitor['name']] = "Found in results"
                else:
                    rankings[competitor['name']] = "Not found"
                    
        return rankings

# ä½¿ç”¨ç¤ºä¾‹
monitor = CompetitorKeywordMonitor("your_api_token")

# æ·»åŠ ç«äº‰å¯¹æ‰‹
monitor.add_competitor("Competitor A", "competitor-a.com")
monitor.add_competitor("Competitor B", "competitor-b.com")

# ç›‘æ§å…³é”®è¯
keywords_to_monitor = [
    "web scraping tools",
    "data extraction services",
    "proxy networks"
]

monitoring_results = monitor.monitor_keywords(keywords_to_monitor)

# ä¿å­˜ç»“æœ
with open("competitor_monitoring.json", "w") as f:
    json.dump(monitoring_results, f, indent=2, ensure_ascii=False)
```

![image-20250914111319974](https://restname.oss-cn-hangzhou.aliyuncs.com/image-20250914111319974.png)

### å®é™…æµ‹è¯•ç»“æœï¼šç«äº‰å¯¹æ‰‹æ’ååˆ†æ

åŸºäºæˆ‘ä»¬çš„å¢å¼ºç‰ˆç›‘æ§ç³»ç»Ÿï¼Œä»¥ä¸‹æ˜¯å®é™…å‘ç°çš„ç«äº‰å¯¹æ‰‹æ’åï¼š

**å…³é”®è¯: "web scraping services"**
- **ScrapingBee**: 8æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬17ä½ âœ…
- **Apify**: 8æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬14ä½ âœ…
- **Bright Data**: 1æ¬¡å‡ºç°
- **Oxylabs**: 1æ¬¡å‡ºç°

**å…³é”®è¯: "data extraction API"**
- **Apify**: 7æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬24ä½ âœ…

**å…³é”®è¯: "proxy network providers"**
- **Bright Data**: 8æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬27ä½ âœ…
- **Oxylabs**: 14æ¬¡å‡ºç°ï¼Œæœ€é«˜æ’åç¬¬28ä½ âœ…

è¿™äº›æ•°æ®å¯¹äºäº†è§£å¸‚åœºç«äº‰æ ¼å±€å’Œä¼˜åŒ–SEOç­–ç•¥éå¸¸æœ‰ä»·å€¼ã€‚





## æ•°æ®å¤„ç†å’Œåˆ†æ

### 1. æå–ç»“æ„åŒ–æ•°æ®

```python
def extract_search_results(raw_results):
    """
    ä»åŸå§‹æœç´¢ç»“æœä¸­æå–ç»“æ„åŒ–æ•°æ®
    """
    parsed = client.parse_content(raw_results)
    
    structured_results = []
    
    # æ ¹æ®å®é™…è¿”å›æ ¼å¼æå–æ•°æ®
    if isinstance(parsed, dict):
        # æå–æ ‡é¢˜ã€URLã€æè¿°ç­‰ä¿¡æ¯
        # è¿™é‡Œçš„å…·ä½“å®ç°å–å†³äºAPIè¿”å›çš„æ•°æ®æ ¼å¼
        if 'text' in parsed:
            # ç®€å•çš„æ–‡æœ¬è§£æç¤ºä¾‹
            lines = parsed['text'].split('\n')
            for line in lines:
                if line.strip():
                    structured_results.append({
                        "content": line.strip(),
                        "type": "text_line"
                    })
    
    return structured_results

# ä½¿ç”¨ç¤ºä¾‹
results = client.search("Python tutorials")
structured_data = extract_search_results(results)
```

### 2. å¯¼å‡ºä¸ºä¸åŒæ ¼å¼

```python
import csv
import pandas as pd

def export_results(results, format="json"):
    """
    å°†æœç´¢ç»“æœå¯¼å‡ºä¸ºä¸åŒæ ¼å¼
    
    Args:
        results: æœç´¢ç»“æœ
        format: å¯¼å‡ºæ ¼å¼ (json, csv, excel)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if format == "json":
        filename = f"search_results_{timestamp}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
    elif format == "csv":
        filename = f"search_results_{timestamp}.csv"
        # å°†ç»“æœè½¬æ¢ä¸ºå¹³é¢ç»“æ„
        flat_results = []
        for item in results:
            flat_results.append({
                "query": item.get("query", ""),
                "timestamp": item.get("timestamp", ""),
                "content": str(item.get("results", ""))[:500]  # é™åˆ¶é•¿åº¦
            })
        
        df = pd.DataFrame(flat_results)
        df.to_csv(filename, index=False, encoding="utf-8")
        
    elif format == "excel":
        filename = f"search_results_{timestamp}.xlsx"
        df = pd.DataFrame(results)
        df.to_excel(filename, index=False)
    
    print(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
    return filename
```



## é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µ

### 1. å®ç°é‡è¯•æœºåˆ¶

```python
import time
from typing import Optional

def search_with_retry(query: str, max_retries: int = 3) -> Optional[dict]:
    """
    å¸¦é‡è¯•æœºåˆ¶çš„æœç´¢å‡½æ•°
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        æœç´¢ç»“æœæˆ–None
    """
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•æœç´¢ ({attempt + 1}/{max_retries}): {query}")
            results = client.search(query)
            return client.parse_content(results)
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5  # é€’å¢ç­‰å¾…æ—¶é—´
                print(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæœç´¢: {query}")
                return None
```

### 2. æ—¥å¿—è®°å½•

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('google_search.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('GoogleSearchScraper')

def logged_search(query):
    """å¸¦æ—¥å¿—è®°å½•çš„æœç´¢"""
    logger.info(f"å¼€å§‹æœç´¢: {query}")
    
    try:
        start_time = time.time()
        results = client.search(query)
        elapsed_time = time.time() - start_time
        
        logger.info(f"æœç´¢æˆåŠŸ: {query} (è€—æ—¶: {elapsed_time:.2f}ç§’)")
        return results
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {query} - {e}")
        raise
```

## æŠ€æœ¯äº®ç‚¹ï¼šMCP Serverä¸è‡ªåŠ¨åŒ–é›†æˆ

### MCP Serverç®€ä»‹

Bright Dataçš„Web MCP Serverï¼ˆModel Context Protocol Serverï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„Webæ•°æ®è®¿é—®APIï¼Œæ”¯æŒï¼š

- **é™æ€ä¸åŠ¨æ€ç½‘é¡µ** - è‡ªåŠ¨å¤„ç†JavaScriptæ¸²æŸ“
- **å…è´¹é¢åº¦** - æ¯æœˆ5,000æ¬¡è¯·æ±‚ï¼Œå‰3ä¸ªæœˆå…è´¹
- **SSEå’ŒHTTP** - æ”¯æŒServer-Sent Eventså’Œæ ‡å‡†HTTPè¯·æ±‚
- **æ™ºèƒ½ä»£ç†** - è‡ªåŠ¨è§£é”ã€è‡ªåŠ¨å¤„ç†éªŒè¯ç 

### ä¸è‡ªåŠ¨åŒ–å·¥å…·é›†æˆ

æ ¹æ®PDFæ–‡æ¡£ï¼ŒBright Dataå¯ä»¥ä¸å¤šç§è‡ªåŠ¨åŒ–å·¥å…·æ— ç¼é›†æˆï¼š

```python
# n8né›†æˆç¤ºä¾‹
{
    "node_type": "HTTP Request",
    "url": "https://api.brightdata.com/mcp/v1/extract",
    "headers": {
        "Authorization": "Bearer YOUR_API_TOKEN"
    },
    "body": {
        "url": "{{search_url}}",
        "browser": true,
        "unlocker": true
    }
}

# æ”¯æŒçš„è‡ªåŠ¨åŒ–å¹³å°ï¼š
# - n8n
# - Zapier  
# - Apache Airflow
# - Make (Integromat)
# - Claude Desktop Extension
# - LangChain
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ä½¿ç”¨å¼‚æ­¥è¯·æ±‚**
   ```python
   results = client.search(query=queries, async_request=True)
   ```

2. **æ§åˆ¶å¹¶å‘æ•°**
   ```python
   results = client.search(query=queries, max_workers=10)
   ```

3. **å®ç°ç¼“å­˜æœºåˆ¶**
   ```python
   from functools import lru_cache
   
   @lru_cache(maxsize=100)
   def cached_search(query):
       return client.search(query)
   ```

## æ³¨æ„äº‹é¡¹

1. **éµå®ˆä½¿ç”¨æ¡æ¬¾** - å§‹ç»ˆéµå®ˆGoogleå’ŒBright Dataçš„æœåŠ¡æ¡æ¬¾
2. **æ§åˆ¶è¯·æ±‚é¢‘ç‡** - é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
3. **ä¿æŠ¤APIä»¤ç‰Œ** - ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
4. **ç›‘æ§ä½¿ç”¨é‡** - æ³¨æ„APIè°ƒç”¨é™é¢

## æ€»ç»“

é€šè¿‡æœ¬æ–‡çš„å®æˆ˜æ¡ˆä¾‹å’Œå®é™…æµ‹è¯•æ•°æ®ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨Bright Data SDKé«˜æ•ˆåœ°æŠ“å–Googleæœç´¢ç»“æœã€‚

### å®æµ‹æ•°æ®æ€»ç»“
âœ… **100%æˆåŠŸç‡** - æ‰€æœ‰æµ‹è¯•è¯·æ±‚éƒ½æˆåŠŸè¿”å›æ•°æ®  
âœ… **ä¸°å¯Œçš„æ•°æ®é‡** - å•æ¬¡æœç´¢è¿”å›40ä¸‡-200ä¸‡å­—ç¬¦  
âœ… **å…¨çƒè¦†ç›–** - æˆåŠŸæµ‹è¯•äº†ç¾å›½ã€è‹±å›½ã€åŠ æ‹¿å¤§ã€æ¾³å¤§åˆ©äºš  
âœ… **ç«å“ç›‘æ§æœ‰æ•ˆ** - å‡†ç¡®è¯†åˆ«äº†ç«äº‰å¯¹æ‰‹æ’åå’Œå‡ºç°é¢‘ç‡  

### æŠ€æœ¯ä¼˜åŠ¿
- **ç®€å•æ˜“ç”¨** - ä½¿ç”¨Python SDKå‡ è¡Œä»£ç å³å¯å¼€å§‹
- **è‡ªåŠ¨åŒ–é›†æˆ** - æ”¯æŒn8nã€Zapierã€Airflowç­‰ä¸»æµå·¥å…·
- **AIåº”ç”¨æ”¯æŒ** - å¯ç”¨äºAI Agentè‡ªåŠ¨åŒ–å·¥ä½œæµ
- **å…è´¹è¯•ç”¨** - å‰3ä¸ªæœˆå…è´¹ï¼Œæ¯æœˆ5000æ¬¡è¯·æ±‚

### æ¨èç”¨é€”ï¼ˆåŸºäºPDFï¼‰
- ç½‘é¡µæ•°æ®æŠ“å–å’Œå®æ—¶æ•°æ®é‡‡é›†
- æ™ºèƒ½ä½“(AI Agent)è‡ªåŠ¨åŒ–å·¥ä½œæµ
- Pythonçˆ¬è™«å’ŒåŠ¨æ€ç½‘é¡µæŠ“å–
- Claudeæ’ä»¶å’Œn8nè‡ªåŠ¨åŒ–

æ— è®ºæ˜¯å¸‚åœºç ”ç©¶ã€SEOåˆ†æè¿˜æ˜¯ç«å“ç›‘æ§ï¼ŒBright Dataéƒ½æä¾›äº†å¯é çš„è§£å†³æ–¹æ¡ˆã€‚è®°ä½åˆç†ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œéµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œè®©æ•°æ®é‡‡é›†ä¸ºæ‚¨çš„ä¸šåŠ¡åˆ›é€ ä»·å€¼ã€‚

## ç›¸å…³èµ„æº

æ ¹æ®PDFæ–‡æ¡£ï¼Œä»¥ä¸‹æ˜¯å®˜æ–¹èµ„æºé“¾æ¥ï¼š

- ğŸŒ **å®˜æ–¹é¡µé¢**: [https://bright.cn/ai/mcp-server](https://bright.cn/ai/mcp-server)
- ğŸ“š **æŠ€æœ¯æ–‡æ¡£**: [https://docs.brightdata.com/api-reference/MCP-Server](https://docs.brightdata.com/api-reference/MCP-Server)
- ğŸ’» **GitHubç¤ºä¾‹**: [https://github.com/brightdata](https://github.com/brightdata)
- ğŸ“¦ **NPMåŒ…**: [https://www.npmjs.com/org/brightdata](https://www.npmjs.com/org/brightdata)
- ğŸ§ª **æµ‹è¯•é¢åº¦**: [https://get.brightdata.com/afftest](https://get.brightdata.com/afftest)

---

*ä½œè€…ï¼šMaynorAIæŠ€æœ¯åšå®¢å›¢é˜Ÿ*  
*å‘å¸ƒæ—¥æœŸï¼š2025å¹´9æœˆ14æ—¥*  
*æ ‡ç­¾ï¼šPython, Web Scraping, Google Search, Bright Data, API*